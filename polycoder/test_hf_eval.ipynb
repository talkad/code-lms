{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e082bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyvo/anaconda3/envs/codenlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import transformers\n",
    "\n",
    "from transformers import GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b573db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_home = '/home/vyvo/projects/compcoder/checkpoints/hf_conversions/'\n",
    "ckpt_name = 'allc_gpt2tok_2-7B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4ded53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint\n",
    "# NOTE: This has only been tested for transformers==4.23.1, torch==1.12.1\n",
    "model = GPTNeoXForCausalLM.from_pretrained(ckpt_home + ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aaf94c1-42d9-4d9c-a3a1-c90548cf2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-t', '--tokenizer_type', type=str, default='GPT2BPETokenizer')\n",
    "parser.add_argument('-v', '--vocab_file', type=str, default='./megatron/tokenizer/gpt_vocab/gpt2-vocab.json')\n",
    "parser.add_argument('-m', '--merge_file', type=str, default='./megatron/tokenizer/gpt_vocab/gpt2-merges.txt')\n",
    "parser.add_argument('-d', '--data_path', type=str, default=f'{os.path.expanduser(\"~\")}/data/OMP_Dataset/c-cpp/source/')\n",
    "parser.add_argument('--save', type=bool, default=True)\n",
    "# The following arguments are leftover from megatron settings -- you can keep the defaults\n",
    "parser.add_argument('--rank', type=int, default=0)\n",
    "parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128)\n",
    "parser.add_argument('--model_parallel_size', type=int, default=1)\n",
    "\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "157e35ff-0c80-4f63-a89c-69a530f48a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path='/home/vyvo/data/OMP_Dataset/c-cpp/source/', make_vocab_size_divisible_by=128, merge_file='./megatron/tokenizer/gpt_vocab/gpt2-merges.txt', model_parallel_size=1, rank=0, save=True, tokenizer_type='GPT2BPETokenizer', vocab_file='./megatron/tokenizer/gpt_vocab/gpt2-vocab.json')\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f2d2804-e2ae-4ad7-8f11-fbc568a116a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tasks.omp.hf_data_omp' from '/home/vyvo/projects/compcoder/fork_to_update_upstream/code-lms/polycoder/tasks/omp/hf_data_omp.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(data_omp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "374a5785-5e2b-45b0-a397-90ddaedcb88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 2788.77it/s]\n",
      "Extracting data files: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 661.15it/s]\n",
      "Generating train split: 30427 examples [00:00, 204134.98 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████| 24342/24342 [00:20<00:00, 1179.12 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████| 3042/3042 [00:02<00:00, 1197.49 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████| 3043/3043 [00:02<00:00, 1210.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█████████████| 24342/24342 [00:00<00:00, 404093.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████| 3042/3042 [00:00<00:00, 262921.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████| 3043/3043 [00:00<00:00, 272399.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import tasks.omp.hf_data_omp as data_omp\n",
    "\n",
    "# Load dataset. This will build the dataset if it does not exist.\n",
    "traind, vald, testd = data_omp.build_omp_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c673102-18e4-4faa-9ec5-63cda4c5e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████████████████| 1/1 [00:00<00:00, 2872.81it/s]\n",
      "Extracting data files: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 352.37it/s]\n",
      "Generating train split: 16384 examples [00:00, 72427.47 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████| 13107/13107 [01:17<00:00, 168.99 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████| 1639/1639 [00:03<00:00, 426.64 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████| 1638/1638 [00:04<00:00, 402.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█████████████| 13107/13107 [00:00<00:00, 136906.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████| 1639/1639 [00:00<00:00, 118523.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████| 1638/1638 [00:00<00:00, 113864.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import tasks.mpi.hf_data_mpi as data_mpi\n",
    "\n",
    "args.data_path = f'{os.path.expanduser(\"~\")}/data/mpiricalplus/dataset/dataset_saved/'\n",
    "# Load dataset. This will build the dataset if it does not exist.\n",
    "traind, vald, testd = data_mpi.build_mpi_dataset(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
