{
  # GPT-style tokenization (BPE)
  "data-path": "/lm_data/",
  "vocab-file": "data/gpt2-vocab.json",
  "merge-file": "data/gpt2-merges.txt",
  
#  # Tokompiler
#  #"data-path": "HPCorpus_final/fine_tune/c-cpp-polycoder/tokompiler/allc_code_document",
#  "data-path": "HPCorpus_final/fine_tune/fortran/tokompiler/fortran_code_document",
#  "tokenizer-type": "Tokompiler",
#  "vocab-file": "megatron/tokenizer/tokompiler/tokenizer_vocab/vocab.txt",

  "num-workers": 0,
  "pad-data-keys": ["input_ids"],
  "do_train": 1,
  "do_valid": 1,
  "do_test": 1,
  "split": "0,0,100",
  "load": "checkpoints/checkpoints-160M/",
  #"load": "checkpoints/allc_gpt2tok_2-7B", #_lr25e-5",
  #"load": "checkpoints/finetune_allc_gpt2tok_2-7B", #_lr25e-5",
  #"load": "checkpoints/allc_tokompiler_2-7B",
  #"load": "checkpoints/allc_tokompiler_700M",
  "checkpoint_validation_with_forward_pass": False,

  "tensorboard-dir": "tensorboard",
  "log-dir": "logs",
  "use_wandb": False,
}
